{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAb/1G03Kf+0GepijWevnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vagabondboffin/topicModeling4UserStories/blob/main/clusteringUS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iqd6BphWvdMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim.downloader as api\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd # Dataframe for data manipulation\n",
        "pd.set_option('display.colheader_justify', 'center')\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "import nltk # Natural langage toolkit\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "import numpy as np # Matrix manipulation\n",
        "import subprocess # to launch bash commands\n",
        "import re # For Regexs\n",
        "import gensim.downloader as api # Topic modeling library. Provide pre-trained models (e.g. Word2Vec)\n",
        "import json # Json manipulation"
      ],
      "metadata": {
        "id": "0SG7VfK1tr0S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "WSFE_oPVva1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def document_vector(word2vec_model, doc):\n",
        "    \"\"\"\n",
        "    calculer la moyenne des vecteurs des mots pour chaque fonctionnalité\n",
        "    \"\"\"\n",
        "    # supprimer les mots qui ne sont pas dans le vocabulaire du modèle Word2Vec\n",
        "    doc = [word for word in doc if word in word2vec_model.key_to_index]\n",
        "    return np.mean(word2vec_model[doc], axis=0)\n",
        "\n",
        "\n",
        "def getOptimalClusterNumber(features, feature_vectors):\n",
        "    distortions = []\n",
        "    indexdistortions = []\n",
        "    sizeMax = len(features)\n",
        "    for i in range(1, sizeMax):\n",
        "        km = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, verbose=0)\n",
        "        km.fit(feature_vectors)\n",
        "        indexdistortions.append((i,km.inertia_))\n",
        "        distortions.append(km.inertia_)\n",
        "\n",
        "    optimal_k = 0\n",
        "    minDistor = 10000000000000\n",
        "    for i, distor in indexdistortions:\n",
        "        if (minDistor>distor):\n",
        "            optimal_k = i\n",
        "            minDistor = distor\n",
        "    return(optimal_k)\n",
        "\n",
        "def clustering(feature_vectors, features, n_clusters=5):\n",
        "    km = KMeans(n_clusters)\n",
        "    km.fit(feature_vectors)\n",
        "    clusterList = []\n",
        "    # afficher les fonctionnalités dans chaque cluster\n",
        "    for i in range(n_clusters):\n",
        "        print(\"Cluster \", i)\n",
        "        cluster_features = []\n",
        "        for j in range(len(features)):\n",
        "            if km.labels_[j] == i:\n",
        "                cluster_features.append(\" \".join(features[j]))\n",
        "        clusterList.append(cluster_features)\n",
        "    return clusterList\n",
        "\n",
        "def generate_cluster_names(clusters, num_keywords=3):\n",
        "    \"\"\"\n",
        "    Generate cluster names automatically based on the most frequent keywords found in each cluster.\n",
        "    :param clusters: a list of clusters, where each cluster is a list of documents\n",
        "    :param num_keywords: the number of keywords to include in each cluster name\n",
        "    :return: a list of cluster names\n",
        "    \"\"\"\n",
        "    cluster_names = []\n",
        "    for cluster in clusters:\n",
        "        # Combine all documents in the cluster into a single string\n",
        "        cluster_text = ' '.join(cluster)\n",
        "        # Tokenize the cluster text into words\n",
        "        words = nltk.word_tokenize(cluster_text)\n",
        "        # Remove stop words and punctuation\n",
        "        words = [word.lower() for word in words\n",
        "                 if len(word) > 1 and word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "        # Count the frequency of each word\n",
        "        word_counts = collections.Counter(words)\n",
        "        # Select the most frequent keywords\n",
        "        top_keywords = [keyword for keyword, count in word_counts.most_common(num_keywords)]\n",
        "        # Generate the cluster name\n",
        "        cluster_name = ' '.join(top_keywords)\n",
        "        cluster_names.append((cluster_name, cluster))\n",
        "    return cluster_names\n",
        "\n",
        "def loadModel(modelName='word2vec-google-news-300'):\n",
        "    return api.load(modelName)\n",
        "\n",
        "def getClusterNamed(model, features, n_clusters=5):\n",
        "\n",
        "    # vectoriser chaque fonctionnalité\n",
        "    feature_vectors = [document_vector(model, feature) for feature in features]\n",
        "\n",
        "    clusterList = clustering(feature_vectors, features, n_clusters)\n",
        "\n",
        "    return generate_cluster_names(clusterList)\n",
        "\n"
      ],
      "metadata": {
        "id": "glU21zGidcka"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctxRelPerCtxForms(df_ctxFormX, data):\n",
        "\n",
        "    tabRet = []\n",
        "    for indexData, rowData in data.iterrows():\n",
        "        tabContains = []\n",
        "\n",
        "        for indexCtx, rowCtx in df_ctxFormX.iterrows():\n",
        "\n",
        "            if rowCtx[0] in rowData[0]:\n",
        "                tabContains.append(\"x\")\n",
        "            else:\n",
        "                tabContains.append(\"\")\n",
        "        tabRet.append(tabContains)\n",
        "    return tabRet\n",
        "\n",
        "def clusterFeatures(model,features):\n",
        "\n",
        "    feature_vectors = [document_vector(model, feature) for feature in features]\n",
        "    #nbClusters = 6\n",
        "    nbClusters= getOptimalClusterNumber(features, feature_vectors)\n",
        "\n",
        "    if nbClusters > (len(features)/4)+2:\n",
        "        nbClusters = int(len(features)/4)+2\n",
        "    if nbClusters == 0:\n",
        "        nbClusters = 1\n",
        "    return getClusterNamed(model, features, nbClusters)\n"
      ],
      "metadata": {
        "id": "F8CTTODRvZKW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uidT03BVuL7g"
      },
      "outputs": [],
      "source": [
        "# Union between two list\n",
        "def union(lst1, lst2):\n",
        "    final_list = list(set(lst1) | set(lst2))\n",
        "    return final_list\n",
        "\n",
        "# role uniformisation\n",
        "def uniformRoles(roles):\n",
        "    roles = list(map(lambda x: x.replace('admin', 'administrator'), roles))\n",
        "    return roles\n",
        "\n",
        "# Cleaning role\n",
        "def cleanRole(sentence):\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    #sentence = sentence.replace(\"test manager\", \"manager\")\n",
        "    sentence = sentence.replace(\"test manager\", \"tester\")\n",
        "    sentence = sentence.replace(\"test engineer\", \"tester\")\n",
        "    sentence = sentence.replace(\"team member\", \"tester\")\n",
        "    sentence = sentence.replace(\"deactivated user\", \"tester\")\n",
        "    #sentence = sentence.replace(\"test developer\", \"developer\")\n",
        "    sentence = sentence.replace(\"test developer\", \"tester\")\n",
        "\n",
        "    sentence = sentence.replace(\"As \", \"\")\n",
        "    sentence = sentence.replace(\"as \", \"\")\n",
        "    sentence = sentence.replace(\"a \", \"\")\n",
        "    sentence = sentence.replace(\"an \", \"\")\n",
        "    sentence = sentence.replace(\"( \", \"\")\n",
        "    sentence = sentence.replace(\") \", \"\")\n",
        "    sentence = sentence.replace(\"'\", \"\")\n",
        "    sentence = sentence.replace(\",\", \"\")\n",
        "    sentence = sentence.replace(\" \", \"\")\n",
        "    return sentence\n",
        "\n",
        "# Cleaning Feature\n",
        "def cleanFeature(sentence):\n",
        "    sentence = nltk.sent_tokenize(sentence)\n",
        "    sentence = ' '.join(sentence)\n",
        "    sentence = sentence.replace(\"want\", \"\")\n",
        "    sentence = sentence.replace(\"'\", \"\")\n",
        "    sentence = sentence.replace(\",\", \"\")\n",
        "    sentence = sentence.replace(\"(\", \" \")\n",
        "    sentence = sentence.replace(\")\", \" \")\n",
        "    sentence = sentence.replace(\"  \", \" \")\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = sentence.split()\n",
        "    sentence = [word for word in words if word not in stop_words]\n",
        "    sentence = \" \".join(sentence)\n",
        "    return sentence[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Lm3GtriIvkw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_complet = pd.read_csv(\"oneLevelUS4Article.csv\").drop(\"index\", axis=1)\n",
        "df_complet['US_title'] = df_complet['US_title'].str.replace(r', so I.*', '', regex=True)\n",
        "#df_complet['US_title'] = df_complet['US_title'].apply(stem_sentence)\n",
        "df_complet.head()"
      ],
      "metadata": {
        "id": "YIpZOLgTt7e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_US = pd.DataFrame(data={\"US\": df_complet[\"US_title\"]})\n",
        "df_US[\"US\"] = df_US[\"US\"].apply(lambda x: x.replace(\",\", \"\"))\n",
        "df_US.to_csv(\"CSVs/ctxF_\" + \"userStories\" + \"_Example.csv\")\n",
        "df_US\n",
        "\n",
        "featureData = []\n",
        "for us in df_complet[\"US_title\"]:\n",
        "    usSplit = us.split(\"I want\")\n",
        "    featureData.append(cleanFeature((\"JJJ\" + \"\".join(usSplit[1:]))[2:]))\n",
        "df_features = pd.DataFrame(data={\"features\": featureData})\n",
        "df_features"
      ],
      "metadata": {
        "id": "BhbHvzqnukjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "#model = KeyedVectors.load(\"/content/LM.kv\").wv\n",
        "model = api.load('glove-wiki-gigaword-200')\n",
        "# model = api.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "EqYH2BM5u-qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# word_counts = df_features['features'].str.split(expand=True).stack().value_counts()\n",
        "# words_to_remove = word_counts[word_counts > 20].index.tolist()\n",
        "# print(words_to_remove)\n",
        "# df_features['features'] = df_features['features'].apply(lambda row: ' '.join([word for word in row.split() if word not in words_to_remove]))\n",
        "features2Roles = my_array = np.array(ctxRelPerCtxForms(df_roles, df_rolesPerUS))\n",
        "df_features2Roles = pd.DataFrame(my_array, columns=df_roles[\"roles\"], index=df_features[\"features\"])\n",
        "#df_features2Roles\n"
      ],
      "metadata": {
        "id": "jmrCOxzDu_SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjuaUeAW3tZb",
        "outputId": "2ffa6ecd-2799-4064-ab65-1e836c2f9105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  0\n",
            "Cluster  1\n",
            "Cluster  2\n",
            "Cluster  3\n",
            "Cluster  4\n",
            "Cluster  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "df_features2Roles['roleCluster'] = df_features2Roles.apply(lambda row: min([col for col, val in row.items() if val == 'x'], key=len) if 'x' in row.values else '', axis=1)\n",
        "\n",
        "featuresRoles = []\n",
        "for role in df_roles[\"roles\"].tolist():\n",
        "    feat = df_features2Roles[df_features2Roles['roleCluster'] == role].index.tolist()\n",
        "    if(len(feat)>0):\n",
        "        featuresRoles.append(list(map(lambda x: x.split(\" \"), feat)))\n",
        "featuresRoles\n",
        "\n",
        "featuresPerClusters = []\n",
        "for featuresPerRoles in featuresRoles:\n",
        "    featuresPerClusters.append(clusterFeatures(model, featuresPerRoles))"
      ]
    }
  ]
}